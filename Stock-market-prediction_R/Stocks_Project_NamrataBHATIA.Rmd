---
title: "Stock market prediction"
format: 
  html:
    toc: true
    toc-location: left
    page-layout: full
    df-print: kable
    fontsize: 1.0em
    embed-resources: true
---
The objective of this project is to predict stock returns from the panel-type data. 

## Importing libraries

Let us load the data and packages.

```{r, message = F, warning = F}
library(tidyverse)
library(dplyr) # for data manipulation
library(tidyr) # to tidy data
library(ggplot2) # for the plots
library(lightgbm) # for LightGBM model
library(corrplot) # for correlation plot
library(reshape2)
library(forecast)
library(plotly)

```

## Functions used in this notebook

This includes some functions I created to be used in this notebook. 

```{r}
drop_ghg_columns <- function(data) {
    data <- data[, !colnames(data) %in% c("ghg_s1", "ghg_s2", "ghg_s3", "year_month")]
    return(data)
}

z_score_normalization <- function(x) {
  (x - mean(x)) / sd(x)
}

```


## Data wrangling

Let us load the dataset that comes in a RData format.

```{r}
load("data/stocks_clean.RData") # Loading the dataset

df <- stocks_clean # Assigning the dataset to a variable df
  
rm(stocks_clean) # now that df is assigned, remove stocks_clean to save memory
```

### Dataset description

Let us have a first look at the dataset df.

```{r}
dim(df) # checking the dimensions of the dataset
```

```{r}
head(df) # Viewing the first few observations
```
```{r}
tail(df) # Viewing the last few observations 
```

```{r}
sapply(df, class) # Checking the dtypes of each column
```
### Variables definition

- **`ticker (char)`**: Unique identifier for each company listed on the stock exchange, represented by a series of letters.

- **`date (date)`**: Date of recorded value.

- **`price (numeric, dbl)`**: Stock price on the given date.

- **`market_cap (numeric, dbl)`**: Total market value of a company's outstanding shares of stock.

- **`price_to_book (numeric, dbl)`**: Ratio compares a company's market value to its book value, providing insights into how the market values the company versus its actual net asset value.

- **`debt_to_equity (numeric, dbl)`**: Measures a company's financial leverage by comparing its total liabilities to shareholders' equity.

- **`profitability (numeric, dbl)`**: Refers to various metrics that measure a company's ability to generate earnings relative to its revenue, assets, equity, or other financial metrics.

- **`volatility (numeric, dbl)`**: Stock's price volatility, which is a measure of the dispersion of returns for a given stock.

- **`revenue (numeric, dbl)`**: Total income generated by the company from its business activities before any expenses are deducted.

- **`ghg_s1, ghg_s2, ghg_s3 (numeric, dbl)`**: These columns refer to greenhouse gas emissions, categorized by scope 1, scope 2, and scope 3 emissions. Scope 1 covers direct emissions from owned or controlled sources, scope 2 covers indirect emissions from the generation of purchased electricity, and scope 3 includes all other indirect emissions that occur in a company's value chain.

- **`return (numeric, dbl)`**: Stock's return over a specific period. This is the value that we need to predict.



```{r}
# Calculating number of unique companies/tickers

print(paste("Number of unique companies:", length(unique(df$ticker))))
```
```{r}
# Calculating duration

min_date <- min(df$date)
max_date <- max(df$date)
duration_months <- round(as.numeric(max_date - min_date) / 12)
print(paste("Total duration from", min_date, "to", max_date, "that is approximately", duration_months, "months"))
```
```{r}

# Checking if time series is complete

df$year_month <- format(df$date, "%Y-%m")

min_date <- min(df$date)
max_date <- max(df$date)

full_sequence <- seq(from = as.Date(format(min_date, "%Y-%m-01")), 
                     to = as.Date(format(max_date, "%Y-%m-01")), 
                     by = "month")

full_year_month <- format(full_sequence, "%Y-%m")

missing_months <- setdiff(full_year_month, df$year_month)
# Used chatgpt for this code

length(missing_months)


```
The time series is complete.

The dataset contains monthly observations of financial data for 885 US firms (tickers) from December, 1995 to March, 2023 with data for 27 years i.e, ~830 months with ~289k rows and 13 columns.

Checking is any ticker is not available in the last two years of data as that will be useful while splitting.

```{r}

x <- max_date - lubridate::years(2)

df |>
  group_by(ticker) |>
  summarise(last_entry = max(date, na.rm = TRUE)) |>
  filter(last_entry <= x) |>
  pull(ticker)  

```
```{r}
# Removing all rows with "CR US Equity" ticker

df <- df |>
  filter(ticker != "CR US Equity")
```

### Structure and statistics of the dataset

#### Structure

```{r}
str(df[,1:13])
```
We only have numerical data and one date format in this dataset. Also, since the forecasting will be company dependent, ticker is also important. 

#### Statistics

```{r}
statistics <- summary(df)
statistics
```
Points to note:

1. There are outliers present in some of the variables as max is considerably higher than the upper quartile values.
2. There are missing values in most of the columns with high number in all ghg columns.
3. Negative values in profitability and revenue need to be investigated as they could be anomalies.
4. Normalization might be required for the columns due to the high range of values. 

```{r}
cat("Percentage of negative revenue values:", (sum(df$revenue < 0, na.rm = TRUE) / nrow(df)) * 100, "%\n")

cat("Number of tickers with negative revenue:", length(unique(df[df$revenue < 0, ]$ticker)), "\n")

negative_revenue <- aggregate(revenue ~ ticker, data = df, function(x) mean(x < 0) * 100)

# Checking the percentage of values that are negative for each ticker
negative_revenue[negative_revenue$revenue > 0, ]

```


```{r}
aue_df <- df[df$ticker == "AIG US Equity", ]
ggplot(aue_df, aes(x = date, y = revenue)) +
  geom_line(size = 0.2, color = "blue") +
  labs(title = "Revenue Over Time for AIG US Equity",
       x = "Date",
       y = "Revenue") +
  theme_minimal()
```
```{r}
aue_df$date[aue_df$revenue < 0]
```
Apparantely, there was a scandal at this time when the revenue went so low for AIG US Equity. Checking the same for some other company that has about 14% negative values for revenue. 

```{r}

cue_df <- df[df$ticker == "CSWC US Equity", ]
ggplot(cue_df, aes(x = date, y = revenue)) +
  geom_line(size = 0.2, color = "blue") +
  labs(title = "Revenue Over Time for CSWC US Equity",
       x = "Date",
       y = "Revenue") +
  theme_minimal()

```
```{r}
cue_df$date[cue_df$revenue < 0]
```
This is spread across time. 

For the companies where the negative values are less that 2%, I will replace it with 0 and for the rest two companies "MBI US Equity" and "CSWC US Equity", I am going to drop them from the dataset.

```{r}
# Dropping two companies
df <- df |> 
  filter(ticker != "MBI US Equity" & ticker != "CSWC US Equity")
```
```{r}
df <- df |>
  mutate(revenue = ifelse(revenue < 0, 0, revenue))
```


### Missing values and duplicates 

#### Missing values

```{r}
# Calculating the null value percentage in each row
round(colSums(is.na(df)) / nrow(df) * 100, 2)
```

```{r}

# Calculating null values in ghg columns post 2011

subset_df <- df |> 
  subset(date > as.Date("2011-12-31"))

round(colSums(is.na(subset_df)) / nrow(subset_df) * 100, 2)

```


There are a few null values present in market_cap, price_to_book, debt_to_equity, profitability, volatility, revenue columns, however all ghg columns have more than 80% null values. Even though the ghg started in 2011, we still have 70-80% null values for each of the ghg columns. 

```{r}

# Used gpt for this

# Step 1 & 2: Check for non-missing values and summarize
ticker_summary <- subset_df %>% #subset_df is df after 2011
  group_by(ticker) %>%
  summarise(
    has_ghg_s1 = any(!is.na(ghg_s1)),
    has_ghg_s2 = any(!is.na(ghg_s2)),
    has_ghg_s3 = any(!is.na(ghg_s3))
  )

# Step 3: Calculate the percentage of tickers with non-missing values
percentage_ghg_s1 = sum(ticker_summary$has_ghg_s1) / nrow(ticker_summary) * 100
percentage_ghg_s2 = sum(ticker_summary$has_ghg_s2) / nrow(ticker_summary) * 100
percentage_ghg_s3 = sum(ticker_summary$has_ghg_s3) / nrow(ticker_summary) * 100

# Print the results
print(paste("Percentage of tickers with at least one non-missing GHG S1 value:", percentage_ghg_s1))
print(paste("Percentage of tickers with at least one non-missing GHG S2 value:", percentage_ghg_s2))
print(paste("Percentage of tickers with at least one non-missing GHG S3 value:", percentage_ghg_s3))

```
I tried to check whether at least one GHG S1 value is present for each company, so I could impute values for that company with the same value, even though this isn't the right approach as green house gas values change every year and are dependent on various factors for each scope. This still wouldn't work out as for about 40% of the companies still don't have any ghg value for each scope. 

Unable to find a solution for ghg_s1, s2, s3 so I have decided to drop these columns for the purpose of this project.

```{r}
# dropping ghg columns

df_cleaner <- drop_ghg_columns(df)
head(df_cleaner)

```

```{r}

# Checking if null values are specific to a few tickers

tickers_always_null <- df_cleaner |>
  group_by(ticker) |>
  summarise(
    all_na_market_cap = all(is.na(market_cap)),
    all_na_price_to_book = all(is.na(price_to_book)),
    all_na_debt_to_equity = all(is.na(debt_to_equity)),
    all_na_profitability = all(is.na(profitability)),
    all_na_volatility = all(is.na(volatility)),
    all_na_revenue = all(is.na(revenue))
  ) |>
  filter(
    all_na_market_cap | 
    all_na_price_to_book | 
    all_na_debt_to_equity | 
    all_na_profitability | 
    all_na_volatility | 
    all_na_revenue
  ) |>
  pull(ticker)
# tickers_always_null now contains tickers with always null values in specified columns
length(tickers_always_null)

```
There are 40 tickers that have null values for one of the columns always. So for those tickers, there is no way to impute the null values for the purpose of this project. I could impute those with 0, but for this project I would leave it as is as imputing it with any value would add to bias. 

```{r}
tickers_always_null
```
Dropping the data for 40 tickers which is about 4.5% of the tickers. 

```{r}

df_cleanest <- df_cleaner |>
  filter(!(ticker %in% tickers_always_null))

round(colSums(is.na(df_cleanest)) / nrow(df_cleanest) * 100, 4)

```
There are still some missing values present in market_cap, price_to_book, debt_to_equity, profitability, volatility and revenue columns. 

For these values, i will impute them with either the next or previous value for that ticker for that column. 

```{r}

df_cleanmax <- df_cleanest |>
  group_by(ticker) |>
  fill(everything(), .direction = "updown")

round(colSums(is.na(df_cleanmax)) / nrow(df_cleanmax) * 100, 4)

```
There are no missing values in our df_cleanmax dataset.

#### Duplicates

Checking for duplicates. 

```{r}
duplicates <- df_cleanmax[duplicated(df_cleanmax), ]
length(duplicates)
```
There are 10 duplicated rows in the dataset.

```{r}
df_cleanmaxpro <- distinct(df_cleanmax)
```

## Exploratory data analysis

```{r}
stocks <- df_cleanmaxpro
```


### Univariate analysis

```{r, message = F, warning = F}
stocks |>
  ggplot(aes(x = price)) +
  geom_histogram(binwidth = 10, fill = "#69b3a2", color = "#69b3a2", alpha = 0.7) + 
  geom_density(aes(y = ..count..), color = "#404080", size = 1) +  
  labs(title = "Histogram of Price",
       x = "Price") +
  theme_light()

stocks |>
  ggplot(aes(x = price)) +
  geom_boxplot(fill = "#69b3a2", color = "#69b3a2", alpha = 0.7) +   
  labs(title = "Boxplot of Price",
       x = "Price") +
  theme_light() + 
  theme(axis.text.x = element_blank())

```

```{r}
# Function to detect outliers 
detect_outliers <- function(data, variable, threshold = 1.5) {
  q1 <- quantile(data[[variable]], 0.25)
  q3 <- quantile(data[[variable]], 0.75)
  
  iqr <- q3 - q1
  
  lower_bound <- q1 - threshold * iqr
  upper_bound <- q3 + threshold * iqr

  outliers <- data[data[[variable]] < lower_bound | data[[variable]] > upper_bound, "ticker"]
  
  return(outliers)
}

# Detect outliers for the price
outliers <- detect_outliers(stocks, "price")

table(outliers)
length(table(outliers))

```

About half of the tickers have outlier values. Since we have data for different tickers, I will treat these outliers per ticker per variable. 


```{r, message = F, warning = F}
stocks |>
  ggplot(aes(x = market_cap)) +
  geom_histogram(binwidth = 10, fill = "#69b3a2", color = "#69b3a2", alpha = 0.7) + 
  geom_density(aes(y = ..count..), color = "#404080", size = 1) +  
  labs(title = "Histogram of market_cap",
       x = "market_cap") +
  theme_light()

stocks |>
  ggplot(aes(x = market_cap)) +
  geom_boxplot(fill = "#69b3a2", color = "#69b3a2", alpha = 0.7) +   
  labs(title = "Boxplot of market_cap",
       x = "market_cap") +
  theme_light() + 
  theme(axis.text.x = element_blank())

```
```{r, message = F, warning = F}
stocks |>
  ggplot(aes(x = price_to_book)) +
  geom_histogram(binwidth = 10, fill = "#69b3a2", color = "#69b3a2", alpha = 0.7) + 
  geom_density(aes(y = ..count..), color = "#404080", size = 1) +  
  labs(title = "Histogram of price_to_book",
       x = "price_to_book") +
  theme_light()

stocks |>
  ggplot(aes(x = price_to_book)) +
  geom_boxplot(fill = "#69b3a2", color = "#69b3a2", alpha = 0.7) +   
  labs(title = "Boxplot of price_to_book",
       x = "price_to_book") +
  theme_light() + 
  theme(axis.text.x = element_blank())

```
```{r}
cat("Company with the max price_to_book:", stocks$ticker[which.max(stocks$price_to_book)], "\n")
```


```{r}
# Checking the price_to_book box plot for this company
cvm_stocks <- subset(stocks, ticker == "CVM US Equity")

cvm_stocks |>
  ggplot(aes(x = price_to_book)) +
  geom_boxplot(fill = "#69b3a2", color = "#69b3a2", alpha = 0.5) +   
  labs(title = "Boxplot of price_to_book",
       x = "price_to_book") +
  theme_light() + 
  theme(axis.text.x = element_blank())
```
I will replace these extreme values with the median value of price_to_book for this company as this seems to be an outlier. 

```{r}

# Calculating the median price_to_book 
median_p <- median(stocks$price_to_book[stocks$ticker == "CVM US Equity"], na.rm = TRUE)

# Calculating the 90th percentile price_to_book 
percentile_9 <- quantile(stocks$price_to_book[stocks$ticker == "CVM US Equity"], probs = 0.9, na.rm = TRUE)

# Finding the indices of price_to_book values above the 90th percentile 
upper_10_idx <- which(stocks$ticker == "CVM US Equity" & stocks$price_to_book > percentile_9)

# Imputing the upper 10% price_to_book values with the median for CVM US Equity
stocks$price_to_book[upper_10_idx] <- median_p

```


```{r}
stocks |>
  ggplot(aes(x = debt_to_equity)) +
  geom_histogram(binwidth = 10, fill = "#69b3a2", color = "#69b3a2", alpha = 0.7) + 
  geom_density(aes(y = ..count..), color = "#404080", size = 1) +  
  labs(title = "Histogram of debt_to_equity",
       x = "debt_to_equity") +
  theme_light()

stocks |>
  ggplot(aes(x = debt_to_equity)) +
  geom_boxplot(fill = "#69b3a2", color = "#69b3a2", alpha = 0.7) +   
  labs(title = "Boxplot of debt_to_equity",
       x = "debt_to_equity") +
  theme_light() + 
  theme(axis.text.x = element_blank())
```
```{r}
cat("Company with the max debt_to_equity:", stocks$ticker[which.max(stocks$debt_to_equity)], "\n")
```
```{r}
# Checking the debt_to_equity box plot for this company
fue_stocks <- subset(stocks, ticker == "FNMA US Equity")

fue_stocks |>
  ggplot(aes(x = debt_to_equity)) +
  geom_boxplot(fill = "#69b3a2", color = "#69b3a2", alpha = 0.5) +   
  labs(title = "Boxplot of debt_to_equity",
       x = "debt_to_equity") +
  theme_light() + 
  theme(axis.text.x = element_blank())
```
I will replace this value with the median value of debt_to_equity for this company as this seems to be an outlier. 

```{r}

# calculating the median debt_to_equity 
median_dte<- median(stocks$debt_to_equity[stocks$ticker == "FNMA US Equity"], na.rm = TRUE)
max_dte_idx <- which(stocks$ticker == "FNMA US Equity" & stocks$debt_to_equity == max(stocks$debt_to_equity[stocks$ticker == "FNMA US Equity"], na.rm = TRUE))

# Imputing the maximum debt_to_equity value with the median 
stocks$debt_to_equity[max_dte_idx] <- median_dte

```

```{r}
stocks |>
  ggplot(aes(x = debt_to_equity)) +
  geom_histogram(binwidth = 10, fill = "#69b3a2", color = "#69b3a2", alpha = 0.7) + 
  geom_density(aes(y = ..count..), color = "#404080", size = 1) +  
  labs(title = "Histogram of debt_to_equity",
       x = "debt_to_equity") +
  theme_light()

stocks |>
  ggplot(aes(x = debt_to_equity)) +
  geom_boxplot(fill = "#69b3a2", color = "#69b3a2", alpha = 0.7) +   
  labs(title = "Boxplot of debt_to_equity",
       x = "debt_to_equity") +
  theme_light() + 
  theme(axis.text.x = element_blank())

```
It still has a lot of outliers but better than the previous distribution. 


```{r}
stocks |>
  ggplot(aes(x = profitability)) +
  geom_histogram(binwidth = 10, fill = "#69b3a2", color = "#69b3a2", alpha = 0.7) + 
  geom_density(aes(y = ..count..), color = "#404080", size = 1) +  
  labs(title = "Histogram of profitability",
       x = "profitability") +
  theme_light()

stocks |>
  ggplot(aes(x = profitability)) +
  geom_boxplot(fill = "#69b3a2", color = "#69b3a2", alpha = 0.7) +   
  labs(title = "Boxplot of profitability",
       x = "profitability") +
  theme_light() + 
  theme(axis.text.x = element_blank())
```
```{r}

cat("Percentage of negative profitability values:", (sum(stocks$profitability < 0) / nrow(stocks)) * 100, "%\n")

```
Due to my limited knowledge of the domain, after doing some research I checked that profitability can indeed be negative for some companies. Checking if it is specific to a company. 

```{r}
cat("Company with the minimum profitability:", stocks$ticker[which.min(stocks$profitability)], "\n")
```
```{r}
# Checking the profitability box plot for this company
gue_stocks <- subset(stocks, ticker == "GSS US Equity")

gue_stocks |>
  ggplot(aes(x = profitability)) +
  geom_boxplot(fill = "#69b3a2", color = "#69b3a2", alpha = 0.5) +   
  labs(title = "Boxplot of profitability",
       x = "profitability") +
  theme_light() + 
  theme(axis.text.x = element_blank())
```
```{r}
summary(gue_stocks)
```
It's profitability has some outliers. We will impute them with the median value.

```{r}

# Calculating the median profitability for GSS US Equity
median_p <- median(stocks$profitability[stocks$ticker == "GSS US Equity"], na.rm = TRUE)

# Calculating the 15th percentile profitability for GSS US Equity
percentile_15 <- quantile(stocks$profitability[stocks$ticker == "GSS US Equity"], probs = 0.15, na.rm = TRUE)

# Finding the indices of profitability values below the 12th percentile
lower_15_idx <- which(stocks$ticker == "GSS US Equity" & stocks$profitability < percentile_15)

# Imputing the lower 15% profitability values with the median for GSS US Equity
stocks$profitability[lower_15_idx] <- median_p

```
```{r}
# Checking the profitability box plot for this company
gue_stocks <- subset(stocks, ticker == "GSS US Equity")

gue_stocks |>
  ggplot(aes(x = profitability)) +
  geom_boxplot(fill = "#69b3a2", color = "#69b3a2", alpha = 0.5) +   
  labs(title = "Boxplot of profitability",
       x = "profitability") +
  theme_light() + 
  theme(axis.text.x = element_blank())
```
```{r}
# Performing the same process for "MUX US Equity"

# Calculating the median profitability for MUX US Equity
median_p <- median(stocks$profitability[stocks$ticker == "MUX US Equity"], na.rm = TRUE)

# Calculating the 5th percentile profitability for MUX US Equity
percentile_5 <- quantile(stocks$profitability[stocks$ticker == "MUX US Equity"], probs = 0.1, na.rm = TRUE)

# Finding the indices of profitability values below the 5th percentile
lower_5_idx <- which(stocks$ticker == "MUX US Equity" & stocks$profitability < percentile_5)

# Imputing the lower 5% profitability values with the median for MUX US Equity
stocks$profitability[lower_5_idx] <- median_p

```
```{r}
# Checking the profitability box plot for this company
gue_stocks <- subset(stocks, ticker == "MUX US Equity")

gue_stocks |>
  ggplot(aes(x = profitability)) +
  geom_boxplot(fill = "#69b3a2", color = "#69b3a2", alpha = 0.5) +   
  labs(title = "Boxplot of profitability",
       x = "profitability") +
  theme_light() + 
  theme(axis.text.x = element_blank())
```

```{r}
stocks |>
  ggplot(aes(x = volatility)) +
  geom_histogram(binwidth = 10, fill = "#69b3a2", color = "#69b3a2", alpha = 0.7) + 
  geom_density(aes(y = ..count..), color = "#404080", size = 1) +  
  labs(title = "Histogram of volatility",
       x = "volatility") +
  theme_light()

stocks |>
  ggplot(aes(x = volatility)) +
  geom_boxplot(fill = "#69b3a2", color = "#69b3a2", alpha = 0.7) +   
  geom_vline(xintercept = quantile(stocks$volatility, probs = c(0.99991)), color = "red", linetype = "dashed") +
  labs(title = "Boxplot of volatility",
       x = "volatility") +
  theme_light() + 
  theme(axis.text.x = element_blank())
```

```{r}
cat("Company with the max volatility:", stocks$ticker[which.max(stocks$volatility)], "\n")
```
```{r}
# Checking the debt_to_equity box plot for this company
iue_stocks <- subset(stocks, ticker == "IMCI US Equity")

iue_stocks |>
  ggplot(aes(x = volatility)) +
  geom_boxplot(fill = "#69b3a2", color = "#69b3a2", alpha = 0.5) +   
  labs(title = "Boxplot of volatility",
       x = "volatility") +
  theme_light() + 
  theme(axis.text.x = element_blank())
```
```{r}

# Calculating the median volatility 
median_v <- median(stocks$volatility[stocks$ticker == "IMCI US Equity"], na.rm = TRUE)

# Calculating the 90th percentile volatility 
percentile_9 <- quantile(stocks$volatility[stocks$ticker == "IMCI US Equity"], probs = 0.9, na.rm = TRUE)

# Finding the indices of volatility values above the 90th percentile 
upper_10_idx <- which(stocks$ticker == "IMCI US Equity" & stocks$volatility > percentile_9)

# Imputing the upper 10% volatility values with the median for IMCI US Equity
stocks$volatility[upper_10_idx] <- median_v

```



```{r}
stocks |>
  ggplot(aes(x = revenue)) +
  geom_histogram(binwidth = 10, fill = "#69b3a2", color = "#69b3a2", alpha = 0.7) + 
  geom_density(aes(y = ..count..), color = "#404080", size = 1) +  
  labs(title = "Histogram of revenue",
       x = "revenue") +
  theme_light()

stocks |>
  ggplot(aes(x = revenue)) +
  geom_boxplot(fill = "#69b3a2", color = "#69b3a2", alpha = 0.7) +  
  labs(title = "Boxplot of revenue",
       x = "revenue") +
  theme_light() + 
  theme(axis.text.x = element_blank())
```


```{r}
stocks |>
  ggplot(aes(x = return)) +
  geom_histogram(binwidth = 10, fill = "#69b3a2", color = "#69b3a2", alpha = 0.7) + 
  geom_density(aes(y = ..count..), color = "#404080", size = 1) +  
  labs(title = "Histogram of return",
       x = "return") +
  theme_light()

stocks |>
  ggplot(aes(x = return)) +
  geom_boxplot(fill = "#69b3a2", color = "#69b3a2", alpha = 0.7) +   
  labs(title = "Boxplot of return",
       x = "return") +
  theme_light() + 
  theme(axis.text.x = element_blank())
```
Checking if all tickers have the same frequency in the dataset.

```{r}
# counting how many tickers belong to different frequencies
barplot(table(table(stocks$ticker)), main = "Frequency of Ticker Values", 
        xlab = "Ticker", ylab = "Frequency",
        col = "skyblue", border = "black")
```
Majority of the tickers have a frequency of 328 whereas 29 tickers do not have the same frequency. 

```{r}
names(table(stocks$ticker)[table(stocks$ticker)==129])
```
```{r}
summary(subset(stocks, ticker == "HR US Equity"))
```
For example, "HR US Equity" stock's starting year is 2012. I will not remove this data.


### Multivariate analysis

#### Correlation analysis

```{r}
# selecting variables
selected_vars <- c("price", "market_cap", "price_to_book", "debt_to_equity", "profitability", "volatility", "revenue", "return")

# Calculating Spearman correlation (over pearson) as the data is not normally distributed and we can't assume linear relationship between variables especially as they are ticker dependent
correlation_matrix <- cor(stocks[, selected_vars], method = "spearman")

# Melting the correlation matrix for visualization
melted_correlation <- melt(correlation_matrix)

# Rounding the correlation coefficients to two decimal places
melted_correlation$value <- round(melted_correlation$value, 2)

# used gpt for the colors :)
my_palette <- c("#f7fbff", "#deebf7", "#c6dbef", "#9ecae1", "#6baed6", "#4292c6", "#2171b5", "#08519c", "#08306b")

ggplot(melted_correlation, aes(Var1, Var2, fill = value, label = value)) +
  geom_tile(color = "white") +
  geom_text(color = "black") +  # Add text labels
  scale_fill_gradientn(colors = my_palette, name = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.title.x = element_blank(),
        axis.title.y = element_blank()) +
  coord_fixed()

```
The variables do not exhibit a strong correlation with the target variable but with each other. 

#### Numerical vs categorical

Performing kruskal-Wallis test between the categorical variable ticker and the target variable return. Since, the data is not normally distributed we cannot use ANOVA.

```{r}
kruskal_result <- kruskal.test(return ~ ticker, data = stocks)

# Print the test result
print(kruskal_result)
```
The chi-squared test statistic is 988.51 with 841 degrees of freedom, and the p-value is less than the significance level of 0.05, we reject the null hypothesis. 

This indicates that there is evidence to suggest that the median returns differ across the ticker categories.

Hence, there is a statistically significant association between the "return" and "ticker" variables.


### Time series analysis

```{r}
# Aggregating returns by date
agg_returns <- stocks |>
  group_by(date) |> 
  summarise(total_return = sum(return, na.rm = TRUE))

# Plotting aggregate returns over time
gg <- ggplot(agg_returns, aes(date, total_return)) +
  geom_line(col = "blue") +
  labs(x = "Date", y = "Total Return", title = "Aggregate Returns Over Time")

# Convert ggplot to plotly object
ggplotly(gg, dynamicTicks = TRUE)
```
The returns experience significant fluctuations over the period observed. There are sharp increases and decreases, indicating a high degree of volatility. There are periods of positive and negatives.

```{r}
stocks$date <- as.Date(stocks$date)

ts_data <- ts(stocks$return, frequency = 1)

arima_model <- auto.arima(ts_data)

summary(arima_model)

plot(forecast(arima_model))
```
The non-zero mean indicates that there's a long-term average effect that the model accounts for, which is important for return series that often exhibit such characteristics. The low error measures and near-zero first autocorrelation of errors suggest that the model's predictions should be reliable and that the residuals are mostly random, which is a desirable property.

```{r}

ggplot(stocks, aes(x = date, y = return, color = ticker)) +
  geom_line() +
  labs(x = "Date", y = "Return", title = "Return Over Date for different Tickers") +
  theme_minimal()

```
```{r}

set.seed(42)  
random_tickers <- sample(unique(stocks$ticker), 3)

subset_data <- stocks %>% 
  filter(ticker %in% random_tickers)

# Plot
ggplot(subset_data, aes(x = date, y = return, color = ticker)) +
  geom_line() +
  labs(x = "Date", y = "Return", title = "Return Over Date for different tickers")
  
```
All three equities have periods of significant volatility, as evidenced by the sharp peaks and troughs. GSS US Equity, in green, shows particularly high volatility with several spikes surpassing a return of 1.0 and dipping below -0.5.

None of the equities show a clear long-term trend upwards or downwards; they all seem to fluctuate around a return of zero. This could imply that their prices have oscillated around a mean value, or that the returns have been adjusted to remove any trend.

There are several notable outliers, particularly for GSS US Equity, which may represent specific events that caused drastic increases or decreases in return on those dates.

## Machine Learning

Label encoding ticker

```{r}
stocks$ticker <- as.numeric(factor(stocks$ticker))

# Adding month and year variables
stocks$month <- month(stocks$date)
stocks$year <- year(stocks$date)
head(stocks)
```

### Dataset split into train and test

```{r}
# Now I will split the data set into train and test

threshold_date <- max(stocks$date) - 365  

df_train <- subset(stocks, date < threshold_date)
df_test <- subset(stocks, date >= threshold_date)

```


```{r}
x_train <- df_train[, !names(df_train) %in% c("return", "date")]
y_train <- df_train$return
x_test <- df_test[, !names(df_test) %in% c("return", "date")]
y_test <- df_test$return

```

### Feature engineering

```{r}

# Apply z-score normalization by ticker for training data
x_train_scaled <- x_train %>%
  group_by(ticker) %>%
  mutate(across(.cols = everything(), .fns = z_score_normalization)) %>%
  ungroup()

# Apply z-score normalization by ticker for testing data
x_test_scaled <- x_test %>%
  group_by(ticker) %>%
  mutate(across(.cols = everything(), .fns = z_score_normalization)) %>%
  ungroup()
```

### Modelling

#### First model

##### Setting Up Parameters to train

```{r}
train_params <- list(objective = "regression", 
                     boosting_type = "gbdt",
                     boost_from_average = "false",
                     learning_rate = 0.005, # Learning rate
                     num_leaves = 192, # Max nb leaves in tree
                     min_data_in_leaf = 100,
                     feature_fraction = 0.3, # % of features
                     bagging_freq = 1,
                     bagging_fraction = 0.7, # % of observations
                     lambda_l1 = 0,
                     lambda_l2 = 0)
```

##### Training the model

```{r}
bst_k <- lightgbm(
  data = as.matrix(x_train_scaled),
  label = y_train, # Target / label
  params = train_params,        # Passing parameter values
  nrounds = 2000 ,     # Number of boosting rounds
  eval_freq = 20,  # Evaluate the model every 20 rounds
  verbose = -1  # Disable verbose output
)
```

```{r}
# Calculate predictions
predictions <- predict(bst_k, as.matrix(x_test_scaled))

# Calculate RMSE
rmse <- sqrt(mean((predictions - y_test)^2))

# Print RMSE
cat("Root Mean Squared Error (RMSE):", rmse, "\n")
```


Let’s use that last model and perform a cross validation

#### Cross validation

```{r}
cv_model <- lgb.cv(
  params = train_params,
  data = as.matrix(x_train_scaled),
  label = y_train, # Target / label
  eval_freq = 80,
  nrounds = 3,                  
  nfold = 5
)
```

```{r}
cv_model$record_evals
```
Lower values of l2 loss indicate better model performance in terms of minimizing the squared differences between predicted and actual values.A smaller l2 evaluation error indicates less variability in the l2 loss values across different runs. 

Model's performance, as indicated by the l2 loss, is consistent across multiple runs, with relatively low variability in the evaluation error. This consistency is indicative of the stability of the model's predictive performance.

#### Hyper parameter tuning

We set up the first combinations

```{r}
num_leaves <- c(5,30)
learning_rate <- c(0.01, 0.05, 0.2)
pars <- expand.grid(num_leaves, learning_rate)
num_leaves <- pars[,1]
learning_rate <- pars[,2]
```
Next, the training function. Note that some parameters are flexible, others are fixed.
```{r}
train_func <- function(num_leaves, learning_rate, x_train_scaled){
  train_params <- list(             # First, the list of params
    num_leaves = num_leaves,        # Max nb leaves in tree
    learning_rate = learning_rate,  # Learning rate
    objective = "regression",           # Loss function
    max_depth = 3,                  # Max depth of trees
    min_data_in_leaf = 50,          # Nb points in leaf
    bagging_fraction = 0.5,         # % of observations
    feature_fraction = 0.7,         # % of features
    nthread = 4,                    # Parallelization
    force_row_wise = T
  )
  
  # Next we train
  bst <- lightgbm(
    data = as.matrix(x_train_scaled),
    label = y_train, # Target / label
    params = train_params,        # Passing parameter values
    eval_freq = 50,
    nrounds = 1000,
    verbose = -1
  )
  # Next, we record the final loss (depends on the model/loss defined above)
  return(loss = bst$record_evals$train$binary_logloss$eval[[10]]) 
}
train_func(10, 0.1, x_train_scaled) # Testing
```

```{r}
train_func <- function(num_leaves, learning_rate, x_train_scaled, y_train) {
  train_params <- list(
    num_leaves = num_leaves,
    learning_rate = learning_rate,
    objective = "regression",
    max_depth = 3,
    min_data_in_leaf = 50,
    bagging_fraction = 0.5,
    feature_fraction = 0.7,
    nthread = 4,
    force_row_wise = TRUE
  )
  
  bst <- lightgbm(
    data = as.matrix(x_train_scaled),
    label = y_train,
    params = train_params,
    eval_freq = 50,
    nrounds = 1000,
    verbose = -1
  )
  
  # Calculate Mean Squared Error (MSE)
  mse_loss <- mean((y_train - predict(bst, newdata = as.matrix(x_train_scaled)))^2)
  
  # Convert MSE to RMSE
  rmse_loss <- sqrt(mse_loss)
  
  # Return the loss value and the trained model
  return(list(loss = rmse_loss, model = bst))
}

# Call the function with appropriate arguments
result <- train_func(10, 0.1, x_train_scaled, y_train)
print(result)
```
RMSE score is 0.115 so close to our model bst_k.

Finally, we can launch a function that is going to span all free parameters.

```{r}
grd <- pmap(list(num_leaves = num_leaves, learning_rate = learning_rate),     # Parameters for the grid search
            ~ train_func(..1, ..2, x_train_scaled, y_train) %>% 
                set_names(c("loss", "model")),                          # Function on which to apply the grid search
            x_train_scaled = x_train_scaled,           # Non-changing argument (data is fixed)
            y_train = y_train                    # Non-changing argument (labels are fixed)
)
```
```{r}
grd
```

```{r}
gr <- bind_cols(as.data.frame(pars), as_tibble(map_dbl(grd, "loss")))
gr
```
So basically it tells us that the best hyper parameters combinations are
30 Leaves and a lr of 0.20 for a rmse loss score of 0.112


Let’s calculate the predictions again


```{r}
# Calculate predictions
trained_model <- result$model
predictions <- predict(trained_model, as.matrix(x_test_scaled))

# Calculate RMSE
rmse <- sqrt(mean((predictions - y_test)^2))

# Print RMSE
cat("Root Mean Squared Error (RMSE):", rmse, "\n")
```

#### Model interpretability

##### Feature Importances
```{r}
importance <- lgb.importance(trained_model)

ggplot(importance, aes(x = Gain, y = reorder(Feature, Gain))) +
  geom_col(fill = "#22AABB", alpha = 0.7) +
  theme_bw() +
  theme(axis.title.y = element_blank(),
        axis.text.y = element_text(size = 6),
        plot.title = element_text(size = 10)) +
  coord_cartesian(clip = "off", ylim = c(0, length(importance$Feature))) +
  labs(title = "Feature Importance") +
  theme(plot.margin = margin(10, 30, 10, 10))
```

## Conclusion

In conclusion, it is evident that both the year and month variables hold paramount significance, given the financial nature of the data and its temporal dependency. Additionally, volatility exhibits a robust association with fluctuations in returns. Moreover, factors such as the stock's identity, represented by its ticker symbol, and its market capitalization size exert substantial influence on the outcomes. 

However, considering the high correlation among certain variables, their individual importance diminishes as much of their information is already captured by other variables. Moving forward, employing advanced models like Neural Prophet could offer promising avenues for further analysis and predictive modeling, potentially yielding deeper insights into the dataset.


